{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import DataLouder #data segregation\r\n",
    "import os\r\n",
    "\r\n",
    "#to import NN\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import os\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import copy\r\n",
    "import torch.optim as optim\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from torch.utils.data import DataLoader, Dataset\r\n",
    "from torch.cuda.amp import autocast, GradScaler\r\n",
    "from tqdm import tqdm\r\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\r\n",
    "\r\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stage I\r\n",
    "Create a directory & upload data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  New directory for GA\r\n",
    "directory = './datasets/post GA/test'\r\n",
    "\r\n",
    "try:\r\n",
    "    os.mkdir(directory)\r\n",
    "except OSError:\r\n",
    "    print(\"Creation of %s failed.\" % directory)\r\n",
    "else:\r\n",
    "    print(\"Successfully created the directory %s .\" % directory)\r\n",
    "\r\n",
    "\r\n",
    "#CSV file path to top aptamers after MAWS\r\n",
    "path_data = './datasets/postMAWS.csv'\r\n",
    "\r\n",
    "df_data = pd.read_csv(path_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stage II\r\n",
    "Load model and create a DataLouder for latter GA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CustomDataset(Dataset):\r\n",
    "\r\n",
    "    def __init__(self, data, maxlen, with_labels=True, bert_model='albert-base-v2'):\r\n",
    "        self.data = data  # pandas dataframe\r\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model, return_dict=False)  \r\n",
    "        self.maxlen = maxlen\r\n",
    "        self.with_labels = with_labels \r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.data)\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        sent1 = str(self.data.loc[index, 'Sequence1'])\r\n",
    "        sent2 = str(self.data.loc[index, 'Sequence2'])\r\n",
    "\r\n",
    "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\r\n",
    "        encoded_pair = self.tokenizer(sent1, sent2, \r\n",
    "                                      padding='max_length',  # Pad to max_length\r\n",
    "                                      truncation=True,  # Truncate to max_length\r\n",
    "                                      max_length=self.maxlen,  \r\n",
    "                                      return_tensors='pt')  # Return torch.Tensor objects\r\n",
    "        \r\n",
    "        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\r\n",
    "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\r\n",
    "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\r\n",
    "\r\n",
    "        if self.with_labels:  # True if the dataset has labels\r\n",
    "            label = self.data.loc[index, 'Label']\r\n",
    "            return token_ids, attn_masks, token_type_ids, label  \r\n",
    "        else:\r\n",
    "            return token_ids, attn_masks, token_type_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Model(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False):\r\n",
    "        super(SentencePairClassifier, self).__init__()\r\n",
    "        self.bert_layer = AutoModel.from_pretrained(bert_model)\r\n",
    "\r\n",
    "        #  Fix the hidden-state size of the encoder outputs\r\n",
    "        #  More information can be found https://huggingface.co/transformers/pretrained_models.html\r\n",
    "        if bert_model == \"albert-base-v2\":  # 12M parameters\r\n",
    "            hidden_size = 768\r\n",
    "        elif bert_model == \"albert-large-v2\":  # 17M parameters\r\n",
    "            hidden_size = 1024\r\n",
    "        elif bert_model == \"albert-xlarge-v2\":  # 58M parameters\r\n",
    "            hidden_size = 2048\r\n",
    "        elif bert_model == \"albert-xxlarge-v2\":  # 223M parameters\r\n",
    "            hidden_size = 4096\r\n",
    "        elif bert_model == \"roberta-base\":  # 125M parameters\r\n",
    "            hidden_size = 768\r\n",
    "        elif bert_model == \"distilroberta-base\":  # 82M parameters\r\n",
    "            hidden_size = 768\r\n",
    "\r\n",
    "        # Freeze bert layers and only train the classification layer weights\r\n",
    "        if freeze_bert:\r\n",
    "            for p in self.bert_layer.parameters():\r\n",
    "                p.requires_grad = False\r\n",
    "\r\n",
    "        # Classification layer\r\n",
    "        self.cls_layer = nn.Linear(hidden_size, 1)\r\n",
    "        self.dropout = nn.Dropout(p=0.1)\r\n",
    "\r\n",
    "    @autocast()  # run in mixed precision\r\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids):\r\n",
    "        '''\r\n",
    "        Inputs:\r\n",
    "            -input_ids : Tensor  containing token ids\r\n",
    "            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\r\n",
    "            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\r\n",
    "        '''\r\n",
    "\r\n",
    "        # Feeding the inputs to the BERT-based model to obtain contextualized representations\r\n",
    "        cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)\r\n",
    "\r\n",
    "        # Feeding to the classifier layer the last layer hidden-state of the [CLS] token further processed by a\r\n",
    "        # Linear Layer and a Tanh activation. The Linear layer weights were trained from the sentence order prediction (ALBERT) or next sentence prediction (BERT)\r\n",
    "        # objective during pre-training.\r\n",
    "        logits = self.cls_layer(self.dropout(pooler_output))\r\n",
    "\r\n",
    "        return logits"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def set_seed(seed):\r\n",
    "    \"\"\" Set all seeds to make results reproducible \"\"\"\r\n",
    "    torch.manual_seed(seed)\r\n",
    "    torch.cuda.manual_seed_all(seed)\r\n",
    "    torch.backends.cudnn.deterministic = True\r\n",
    "    torch.backends.cudnn.benchmark = False\r\n",
    "    np.random.seed(seed)\r\n",
    "    random.seed(seed)\r\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_probs_from_logits(logits):\r\n",
    "    \"\"\"\r\n",
    "    Converts a tensor of logits into an array of probabilities by applying the sigmoid function\r\n",
    "    \"\"\"\r\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\r\n",
    "    return probs.detach().cpu().numpy()\r\n",
    "\r\n",
    "def test_prediction(net, device, dataloader, with_labels=True, result_file=\"results/output.txt\"):\r\n",
    "    \"\"\"\r\n",
    "    Predict the probabilities on a dataset with or without labels and print the result in a file\r\n",
    "    \"\"\"\r\n",
    "    net.eval()\r\n",
    "    w = open(result_file, 'w')\r\n",
    "    probs_all = []\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        if with_labels:\r\n",
    "            for seq, attn_masks, token_type_ids, _ in tqdm(dataloader):\r\n",
    "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\r\n",
    "                logits = net(seq, attn_masks, token_type_ids)\r\n",
    "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\r\n",
    "                probs_all += probs.tolist()\r\n",
    "        else:\r\n",
    "            for seq, attn_masks, token_type_ids in tqdm(dataloader):\r\n",
    "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\r\n",
    "                logits = net(seq, attn_masks, token_type_ids)\r\n",
    "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\r\n",
    "                probs_all += probs.tolist()\r\n",
    "\r\n",
    "    w.writelines(str(prob)+'\\n' for prob in probs_all)\r\n",
    "    w.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bert_model = \"albert-base-v2\"  # any of previously defined BERT alternatives :'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2', 'albert-xxlarge-v2' and others\r\n",
    "maxlen = 32  # maximum length of the tokenized input sentence pair : if greater than \"maxlen\", the input is truncated and else if smaller, the input is padded\r\n",
    "bs = 64  # batch size\r\n",
    "iters_to_accumulate = 2  # the gradient accumulation adds gradients over an effective batch of size : bs * iters_to_accumulate. If set to \"1\", you get the usual batch size\r\n",
    "lr = 1e-4  # learning rate"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def predict()\r\n",
    "\r\n",
    "    path_to_model = '/content/models/albertLarge8.pt'  \r\n",
    "    path_to_output_file = 'results/output.txt'\r\n",
    "\r\n",
    "    print(\"Reading test data...\")\r\n",
    "    #LOAD APTAMERS HERE\r\n",
    "    test_set = CustomDataset(df_test, maxlen, bert_model)\r\n",
    "    test_loader = DataLoader(test_set, batch_size=bs, num_workers=4)\r\n",
    "\r\n",
    "    model = SentencePairClassifier(bert_model)\r\n",
    "    if torch.cuda.device_count() > 1:  # if multiple GPUs\r\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\r\n",
    "        model = nn.DataParallel(model)\r\n",
    "\r\n",
    "    print()\r\n",
    "    print(\"Loading the weights of the model...\")\r\n",
    "    model.load_state_dict(torch.load(path_to_model))\r\n",
    "    model.to(device)\r\n",
    "\r\n",
    "    print(\"Predicting on test data...\")\r\n",
    "    test_prediction(net=model, device=device, dataloader=test_loader, with_labels=True,  # set the with_labels parameter to False if your want to get predictions on a dataset without labels\r\n",
    "                    result_file=path_to_output_file)\r\n",
    "    print()\r\n",
    "    print(\"Predictions are available in : {}\".format(path_to_output_file))                         "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stage III\r\n",
    "Apply Genetic Algorithm to generate new population of aptamers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def run_GA(path1):\r\n",
    "\r\n",
    "    #  Generate N aptamers to have the same 1000 as before deleting inferior\r\n",
    "    %run breeder.py --p PATH1 --o OUTPUT1 --l lenght\r\n",
    "    #increase iteraction by +1 with .format()\r\n",
    "\r\n",
    "    #  Pair up new batch\r\n",
    "    %run pairing.py --p OUTPUT1 --o OUTPUT2\r\n",
    "\r\n",
    "    #  Call alBERT to compare goodness of sequences\r\n",
    "    model.eval() #tikriausiai nereikia\r\n",
    "    df_test = pd.read_csv(OUTPUT2)\r\n",
    "    test_set = CustomDataset(df_test, maxlen, bert_model)\r\n",
    "    data_toModel = DataLouder(test_set) #nureadinti data pirma\r\n",
    "    test_prediction(net=model, device=device, dataloader=data_toModel, with_labels=False,\r\n",
    "                result_file=OUTPUT3)\r\n",
    "\r\n",
    "\r\n",
    "    #  Find dominating aptamers and go to step 1 again.\r\n",
    "    %run dominance_score.py --p OUTPUT3 --o OUTPUT4\r\n",
    "\r\n",
    "#after the final iteration move"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have to determine some conditions to stop while loop and GA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "set_seed(2)\r\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "net = Model(bert_model, freeze_bert=freeze_bert)\r\n",
    "\r\n",
    "if torch.cuda.device_count() > 1:  # if multiple GPUs\r\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\r\n",
    "    net = nn.DataParallel(net)\r\n",
    "\r\n",
    "net.to(device)\r\n",
    "   \r\n",
    "criterion = nn.BCEWithLogitsLoss()\r\n",
    "opti = AdamW(net.parameters(), lr=lr, weight_decay=1e-2)\r\n",
    "num_warmup_steps = 10000 # The number of steps for the warmup phase.\r\n",
    "num_training_steps = epochs * len(train_loader)  # The total number of training steps\r\n",
    "t_total = (len(train_loader) // iters_to_accumulate) * epochs  # Necessary to take into account Gradient accumulation\r\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)\r\n",
    "\r\n",
    "\r\n",
    "while true:#convergency\r\n",
    "    run_GA(first_100_aptamer_path)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "3aee3b0eb6142d763d7754525c5291c780d2cfe01d72d8f6b94276ca950be07b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}