{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#  Dependencies and Paths"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  New directory for current genetic algorithm\r\n",
    "directory = './datasets/runs/test/'\r\n",
    "\r\n",
    "#  Path to scored aptamers\r\n",
    "aptamerList = './datasets/training/best_0.csv'\r\n",
    "aptamerListAll = './datasets/training/scored_sequences.csv'\r\n",
    "#  Path to PyTorch alBERT model\r\n",
    "path_to_model = './model/Albert-base-20epoch-val0-4-run2.pt'  \r\n",
    "\r\n",
    "#  How many sequences we want to have in a list\r\n",
    "apt_len = 1000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  Create a new directory\r\n",
    "try:\r\n",
    "    os.mkdir(directory)\r\n",
    "except OSError:\r\n",
    "    print(\"Creation of %s failed.\" % directory)\r\n",
    "else:\r\n",
    "    print(\"Successfully created the directory %s .\" % directory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\r\n",
    "import os\r\n",
    "import copy\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "from torch.utils.data import DataLoader, Dataset\r\n",
    "from torch.cuda.amp import autocast, GradScaler\r\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\r\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stage I\r\n",
    "Load model and create a DataLouder for latter GA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CustomDataset(Dataset):\r\n",
    "\r\n",
    "    def __init__(self, data, maxlen, with_labels=True, bert_model='albert-base-v2'):\r\n",
    "        self.data = data  # pandas dataframe\r\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model, return_dict=False)  \r\n",
    "        self.maxlen = maxlen\r\n",
    "        self.with_labels = with_labels \r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.data)\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        sent1 = str(self.data.loc[index, 'Sequence1'])\r\n",
    "        sent2 = str(self.data.loc[index, 'Sequence2'])\r\n",
    "\r\n",
    "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\r\n",
    "        encoded_pair = self.tokenizer(sent1, sent2, \r\n",
    "                                      padding='max_length',  # Pad to max_length\r\n",
    "                                      truncation=True,  # Truncate to max_length\r\n",
    "                                      max_length=self.maxlen,  \r\n",
    "                                      return_tensors='pt')  # Return torch.Tensor objects\r\n",
    "        \r\n",
    "        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\r\n",
    "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\r\n",
    "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\r\n",
    "\r\n",
    "        if self.with_labels:  # True if the dataset has labels\r\n",
    "            label = self.data.loc[index, 'Label']\r\n",
    "            return token_ids, attn_masks, token_type_ids, label  \r\n",
    "        else:\r\n",
    "            return token_ids, attn_masks, token_type_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Model(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False):\r\n",
    "        super(Model, self).__init__()\r\n",
    "        self.bert_layer = AutoModel.from_pretrained(bert_model, return_dict=False)\r\n",
    "\r\n",
    "        if bert_model == \"albert-base-v2\":  # 12M parameters\r\n",
    "            hidden_size = 768\r\n",
    "        elif bert_model == \"albert-large-v2\":  # 17M parameters\r\n",
    "            hidden_size = 1024\r\n",
    "        elif bert_model == \"albert-xlarge-v2\":  # 58M parameters\r\n",
    "            hidden_size = 2048\r\n",
    "        elif bert_model == \"albert-xxlarge-v2\":  # 223M parameters\r\n",
    "            hidden_size = 4096\r\n",
    "        elif bert_model == \"roberta-base\":  # 125M parameters\r\n",
    "            hidden_size = 768\r\n",
    "        elif bert_model == \"distilroberta-base\":  # 82M parameters\r\n",
    "            hidden_size = 768\r\n",
    "        #  More information on available models can be found at https://huggingface.co/transformers/pretrained_models.html\r\n",
    "        \r\n",
    "        # Freeze model layers and only train the classification layer weights\r\n",
    "        if freeze_bert:\r\n",
    "            for p in self.bert_layer.parameters():\r\n",
    "                p.requires_grad = False\r\n",
    "\r\n",
    "        # Putting Classification layer on top of BERT\r\n",
    "        self.cls_layer = nn.Linear(hidden_size, 1)\r\n",
    "        self.dropout = nn.Dropout(p=0.1)\r\n",
    "\r\n",
    "    @autocast()  # Mixes precision\r\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids):\r\n",
    "        '''\r\n",
    "        Inputs:\r\n",
    "            -input_ids : Tensor  containing token ids\r\n",
    "            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\r\n",
    "            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\r\n",
    "        '''\r\n",
    "\r\n",
    "        # Feeding the inputs to the BERT-based model to obtain contextualized representations\r\n",
    "        cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)\r\n",
    "\r\n",
    "        # Feeding to the classifier layer the last layer hidden-state of the [CLS] token further processed by a\r\n",
    "        # Linear Layer and a Tanh activation. The Linear layer weights were trained from the sentence order prediction (ALBERT) or next sentence prediction (BERT)\r\n",
    "        # objective during pre-training.\r\n",
    "        logits = self.cls_layer(self.dropout(pooler_output))\r\n",
    "\r\n",
    "        return logits"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def set_seed(seed):\r\n",
    "    \"\"\" Set all seeds to make results reproducible \"\"\"\r\n",
    "    torch.manual_seed(seed)\r\n",
    "    torch.cuda.manual_seed_all(seed)\r\n",
    "    torch.backends.cudnn.deterministic = True\r\n",
    "    torch.backends.cudnn.benchmark = False\r\n",
    "    np.random.seed(seed)\r\n",
    "    random.seed(seed)\r\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_probs_from_logits(logits):\r\n",
    "    \"\"\"\r\n",
    "    Converts a tensor of logits into an array of probabilities by applying the sigmoid function\r\n",
    "    \"\"\"\r\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\r\n",
    "    return probs.detach().cpu().numpy()\r\n",
    "\r\n",
    "def test_prediction(net, device, aptamerDataFrame, dataloader, with_labels, result_path):\r\n",
    "    \"\"\"\r\n",
    "    Predict the probabilities on a dataset with or without labels and print the result in a file\r\n",
    "    \"\"\"\r\n",
    "    net.eval()\r\n",
    "    probs_all = []\r\n",
    "    nb_iterations = len(dataloader)\r\n",
    "    \r\n",
    "    with torch.no_grad():\r\n",
    "        if with_labels:\r\n",
    "            for it, (seq, attn_masks, token_type_ids) in tqdm(enumerate(dataloader), total = nb_iterations):\r\n",
    "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\r\n",
    "                logits = net(seq, attn_masks, token_type_ids)\r\n",
    "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\r\n",
    "                probs_all += probs.tolist()\r\n",
    "\r\n",
    "                \r\n",
    "        else:\r\n",
    "            for it, (seq, attn_masks, token_type_ids) in tqdm(enumerate(dataloader), total=nb_iterations):\r\n",
    "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\r\n",
    "                logits = net(seq, attn_masks, token_type_ids)\r\n",
    "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\r\n",
    "                probs_all += probs.tolist()\r\n",
    "\r\n",
    "                \r\n",
    "    df1 = pd.read_csv(columns=['Sequence1', 'Sequence2'])\r\n",
    "    probs_all = [round(x) for x in probs_all]\r\n",
    "    df2 = pd.DataFrame({'Label': probs_all})\r\n",
    "    df = pd.concat([df1, df2], axis=1)\r\n",
    "    df.to_csv(result_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bert_model = \"albert-base-v2\"  #  'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2' and others\r\n",
    "maxlen = 32                    #  maximum length of the tokenized input sentence pair : if greater than \"maxlen\", the input is truncated and else if smaller, the input is padded\r\n",
    "bs = 64                        #  batch size of testing\r\n",
    "with_labels = False\r\n",
    "iter = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stage III\r\n",
    "Apply Genetic Algorithm to generate new population of aptamers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def run_GA():\r\n",
    "\r\n",
    "    #  Generate N aptamers to have the same 1000 as before deleting inferior\r\n",
    "    !python /content/genetic_algorithm/breeder.py --p {aptamerList} --o {directory} --l {apt_len} --i {iter}\r\n",
    "\r\n",
    "    #  Pair up new batch\r\n",
    "    !python /content/functions/pairing.py --p {aptamerList} --o {directory} --i {iter}\r\n",
    "\r\n",
    "    #  Call alBERT to compare goodness of sequences\r\n",
    "    df_test = pd.read_csv('{}iteration_{}.csv'.format(directory, iter))\r\n",
    "    test_set = CustomDataset(df_test, maxlen, with_labels, bert_model)\r\n",
    "    data_toModel = DataLoader(test_set) #nureadinti data pirma\r\n",
    "    test_prediction(net=model, device=device, aptamerDataFrame='{}iteration_{}'.format(directory, iter), dataloader=data_toModel, with_labels=False, result_path='{}predicted_{}'.format(directory, iter))\r\n",
    "\r\n",
    "\r\n",
    "    #  Find dominating aptamers and go to step 1 again.\r\n",
    "    !python /content/functions/dominance_score.py --p {directory + 'predicted_' +str(iter)} --f {directory + 'breed_' + str(iter) + '.csv'} --o {directory + 'best_' + str(iter) + '.csv'}  --i {iter} --l {apt_len}\r\n",
    "    #survarkyti kur galunes nera tokios ir tegul patys scriptai tuo rupinasi\r\n",
    "\r\n",
    "    aptamerList = directory + 'best_' + iter\r\n",
    "    iter += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "set_seed(2020)\r\n",
    "\r\n",
    "print(\"Loading model...\")\r\n",
    "model.eval() #tikriausiai nereikia\r\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "model = Model(bert_model, freeze_bert=freeze_bert, strict=False)\r\n",
    "model.to(device)\r\n",
    "model.eval() #tikriausiai nereikia\r\n",
    "\r\n",
    "while true:#convergency\r\n",
    "    run_GA(aptamers)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "3aee3b0eb6142d763d7754525c5291c780d2cfe01d72d8f6b94276ca950be07b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}