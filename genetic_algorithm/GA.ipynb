{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "3aee3b0eb6142d763d7754525c5291c780d2cfe01d72d8f6b94276ca950be07b"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit (windows store)",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIzRrgE9VdbB"
      },
      "source": [
        "#  Dependencies and Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WibiFhDJYxnD",
        "outputId": "fc6965da-dc0e-4058-d847-4f15bb70e1dc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnLZ47C_aSbC",
        "outputId": "814b1b19-912b-49df-e500-5a4523989e18"
      },
      "source": [
        "%cd '/content/drive/MyDrive/transformers/gitas/GA_Transformer-main(2).zip (Unzipped Files)/GA_Transformer-main'\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/transformers/gitas/GA_Transformer-main(2).zip (Unzipped Files)/GA_Transformer-main\n",
            "datasets   genetic_algorithm  LICENSE.txt  README.md\t     to_do.txt\n",
            "functions  images\t      model\t   requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYuWYEM0VoyV",
        "outputId": "39363e92-fea1-4193-ae24-481e5b8cd6a8"
      },
      "source": [
        "!pip install transformers==4.9.1 ruamel.yaml"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.9.1\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 7.3 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml\n",
            "  Downloading ruamel.yaml-0.17.16-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 63.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 54.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1) (3.0.12)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 59.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1) (4.62.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 60.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1) (4.6.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers==4.9.1) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.9.1) (2.4.7)\n",
            "Collecting ruamel.yaml.clib>=0.1.2\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 63.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.9.1) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.1) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.1) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.1) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.1) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.1) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, ruamel.yaml.clib, pyyaml, huggingface-hub, transformers, ruamel.yaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 ruamel.yaml-0.17.16 ruamel.yaml.clib-0.2.6 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aifQ6tGqVdbH"
      },
      "source": [
        "import random\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "import ruamel.yaml"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T7z6GOlVdbN"
      },
      "source": [
        "config_name = './genetic_algorithm/settings.yaml'\n",
        "\n",
        "with open(config_name, 'r') as stream:\n",
        "    try:\n",
        "        yaml = ruamel.yaml.YAML()\n",
        "        config = yaml.load(stream)\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)\n",
        "\n",
        "#  New directory for current genetic algorithm\n",
        "directory = config['Paths']['iteration_folder']\n",
        "\n",
        "#  Path to scored aptamers\n",
        "aptamerList = config['Paths']['path_to_initial_aptamers']\n",
        "aptamerListAll = config['Paths']['path_to_all_aptamers']\n",
        "#  Path to PyTorch alBERT model\n",
        "path_to_model = config['Paths']['path_to_model']\n",
        "\n",
        "#  How many sequences we want to have in a list\n",
        "apt_len = config['Parameters']['aptamer_len']\n",
        "\n",
        "aptamerList_iter = './datasets/ga_interim_data/Albumin/breed_1.csv'"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz6g0s47VdbR"
      },
      "source": [
        "Stage I\n",
        "Load model and create a DataLouder for latter GA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny955MP9VdbS"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, maxlen, with_labels=True, bert_model='albert-base-v2'):\n",
        "        self.data = data  # pandas dataframe\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model, return_dict=False)  \n",
        "        self.maxlen = maxlen\n",
        "        self.with_labels = with_labels \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sent1 = str(self.data.loc[index, 'Sequence1'])\n",
        "        sent2 = str(self.data.loc[index, 'Sequence2'])\n",
        "\n",
        "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
        "        encoded_pair = self.tokenizer(sent1, sent2, \n",
        "                                      padding='max_length',  # Pad to max_length\n",
        "                                      truncation=True,  # Truncate to max_length\n",
        "                                      max_length=self.maxlen,  \n",
        "                                      return_tensors='pt')  # Return torch.Tensor objects\n",
        "        \n",
        "        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n",
        "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
        "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
        "\n",
        "        if self.with_labels:  # True if the dataset has labels\n",
        "            label = self.data.loc[index, 'Label']\n",
        "            return token_ids, attn_masks, token_type_ids, label  \n",
        "        else:\n",
        "            return token_ids, attn_masks, token_type_ids"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVDF_6wsVdbU"
      },
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False):\n",
        "        super(Model, self).__init__()\n",
        "        self.bert_layer = AutoModel.from_pretrained(bert_model, return_dict=False)\n",
        "\n",
        "        bert_model == \"albert-base-v2\"  # 12M parameters\n",
        "        hidden_size = 768\n",
        "        #  More information on available models can be found at https://huggingface.co/transformers/pretrained_models.html\n",
        "        \n",
        "        # Freeze model layers and only train the classification layer weights\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        # Putting Classification layer on top of BERT\n",
        "        self.cls_layer = nn.Linear(hidden_size, 1)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    @autocast()  # Mixes precision\n",
        "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -input_ids : Tensor  containing token ids\n",
        "            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n",
        "            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n",
        "        '''\n",
        "\n",
        "        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n",
        "        cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)\n",
        "\n",
        "        # Feeding to the classifier layer the last layer hidden-state of the [CLS] token further processed by a\n",
        "        # Linear Layer and a Tanh activation. The Linear layer weights were trained from the sentence order prediction (ALBERT) or next sentence prediction (BERT)\n",
        "        # objective during pre-training.\n",
        "        logits = self.cls_layer(self.dropout(pooler_output))\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIfXHMI8VdbW"
      },
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\" Set all seeds to make results reproducible \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PmAebATVdbX"
      },
      "source": [
        "def get_probs_from_logits(logits):\n",
        "    \"\"\"\n",
        "    Converts a tensor of logits into an array of probabilities by applying the sigmoid function\n",
        "    \"\"\"\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    return probs.detach().cpu().numpy()\n",
        "\n",
        "def test_prediction(net, device, aptamerDataFrame, dataloader, with_labels, result_path, iteration):\n",
        "    \"\"\"\n",
        "    Predict the probabilities on a dataset with or without labels and print the result in a file\n",
        "    \"\"\"\n",
        "    net.eval()\n",
        "    probs_all = []\n",
        "    nb_iterations = len(dataloader)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        if with_labels:\n",
        "            for it, (seq, attn_masks, token_type_ids) in tqdm(enumerate(dataloader), total = nb_iterations):\n",
        "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
        "                logits = net(seq, attn_masks, token_type_ids)\n",
        "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
        "                probs_all += probs.tolist()\n",
        "\n",
        "                \n",
        "        else:\n",
        "            for it, (seq, attn_masks, token_type_ids) in tqdm(enumerate(dataloader), total=nb_iterations):\n",
        "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
        "                logits = net(seq, attn_masks, token_type_ids)\n",
        "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
        "                probs_all += probs.tolist()\n",
        "\n",
        "                \n",
        "    df1 = pd.read_csv(aptamerDataFrame)\n",
        "    probs_all = [round(x) for x in probs_all]\n",
        "    df2 = pd.DataFrame({'Label': probs_all})\n",
        "    df = pd.concat([df1, df2], axis=1)\n",
        "    df.to_csv(result_path)\n",
        "    \n",
        "    print(\"Compared aptamers iteration {} is located in {}\".format(iteration, result_path))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR33Azc3VdbX"
      },
      "source": [
        "bert_model =  config['Model']['model_name']  #  'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2' and others\n",
        "maxlen =  config['Model']['max_len']         #  maximum length of the tokenized input sentence pair : if greater than \"maxlen\", the input is truncated and else if smaller, the input is padded\n",
        "bs =  config['Model']['batch_size']          #  batch size of testing\n",
        "with_labels =  config['Model']['with_labels']\n",
        "iter = 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD29WGchVdbY"
      },
      "source": [
        "Stage III\n",
        "Apply Genetic Algorithm to generate new population of aptamers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKS3yQJpVdbY"
      },
      "source": [
        "def run_GA():\n",
        "    iter = 1\n",
        "    while iter < 51:\n",
        "      #  Generate N aptamers to have the same 1000 as before deleting inferior\n",
        "      !python ./genetic_algorithm/breeder.py --p {aptamerList} --o {directory} --l {apt_len} --i {iter}\n",
        "      \n",
        "      if iter > 1:\n",
        "        breedCSV = './datasets/ga_interim_data/Albumin/breed_{}.csv'.format(iter-1)\n",
        "        %rm $breedCSV\n",
        "\n",
        "      #  Pair up new batch\n",
        "      !python ./functions/pairing.py --h {aptamerList_iter} --o {directory} --i {iter}\n",
        "\n",
        "      #  Call alBERT to compare goodness of sequences\n",
        "      df_test = pd.read_csv('{}iteration_{}.csv'.format(directory, iter))\n",
        "      test_set = CustomDataset(df_test, maxlen, with_labels, bert_model)\n",
        "      data_toModel = DataLoader(test_set, batch_size=bs) #nureadinti data pirma\n",
        "      test_prediction(net=model, device=device, aptamerDataFrame='{}iteration_{}.csv'.format(directory, iter), dataloader=data_toModel, with_labels=False, result_path='{}predicted_{}.csv'.format(directory, iter), iteration=iter)\n",
        "\n",
        "      #  Find dominating aptamers and go to step 1 again.\n",
        "      !python ./functions/dominance_score.py --p {directory + 'predicted_' +str(iter) + '.csv'} --f {directory + 'breed_' + str(iter) + '.csv'} --o {directory + 'top_iter_' + str(iter)}  --i {iter} --l {apt_len}\n",
        "      #survarkyti kur galunes nera tokios ir tegul patys scriptai tuo rupinasi\n",
        "\n",
        "      iterationCSV = './datasets/ga_interim_data/Albumin/iteration_{}.csv'.format(iter)\n",
        "      predictionCSV = './datasets/ga_interim_data/Albumin/predicted_{}.csv'.format(iter)\n",
        "\n",
        "      aptamerList = directory + 'top_iter_' + str(iter) + '.csv'\n",
        "      iter += 1\n",
        "      aptamerList_iter = './datasets/ga_interim_data/Albumin/breed_{}.csv'.format(iter)\n",
        "\n",
        "      !rm $iterationCSV $predictionCSV "
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyKSDhrbVdbZ"
      },
      "source": [
        "set_seed(2021)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model(bert_model, freeze_bert=False)\n",
        "model.load_state_dict(torch.load(config['Paths']['path_to_model']))\n",
        "model.to(device)\n",
        "model.eval() #tikriausiai nereikia\n",
        "\n",
        "run_GA()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}