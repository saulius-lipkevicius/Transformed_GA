{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIzRrgE9VdbB"
   },
   "source": [
    "#  Dependencies and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYuWYEM0VoyV",
    "outputId": "39363e92-fea1-4193-ae24-481e5b8cd6a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.9.1 in /home/ubuntu/pyenv/lib/python3.8/site-packages (4.9.1)\n",
      "Requirement already satisfied: ruamel.yaml in /home/ubuntu/pyenv/lib/python3.8/site-packages (0.17.16)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/pyenv/lib/python3.8/site-packages (from transformers==4.9.1) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/pyenv/lib/python3.8/site-packages (from transformers==4.9.1) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/pyenv/lib/python3.8/site-packages (from transformers==4.9.1) (4.62.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ubuntu/pyenv/lib/python3.8/site-packages (from transformers==4.9.1) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in /home/ubuntu/pyenv/lib/python3.8/site-packages (from transformers==4.9.1) (0.0.45)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /home/ubuntu/pyenv/lib/python3.8/site-packages (from transformers==4.9.1) (0.0.12)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/pyenv/lib/python3.8/site-packages (from transformers==4.9.1) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/pyenv/lib/python3.8/site-packages (from transformers==4.9.1) (1.21.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/pyenv/lib/python3.8/site-packages (from transformers==4.9.1) (2021.8.21)\n",
      "Requirement already satisfied: requests in /home/ubuntu/pyenv/lib/python3.8/site-packages (from transformers==4.9.1) (2.26.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.10\" in /home/ubuntu/pyenv/lib/python3.8/site-packages (from ruamel.yaml) (0.2.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ubuntu/pyenv/lib/python3.8/site-packages (from packaging->transformers==4.9.1) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ubuntu/pyenv/lib/python3.8/site-packages (from sacremoses->transformers==4.9.1) (1.16.0)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/pyenv/lib/python3.8/site-packages (from sacremoses->transformers==4.9.1) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ubuntu/pyenv/lib/python3.8/site-packages (from sacremoses->transformers==4.9.1) (8.0.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/pyenv/lib/python3.8/site-packages (from huggingface-hub==0.0.12->transformers==4.9.1) (3.10.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/ubuntu/pyenv/lib/python3.8/site-packages (from requests->transformers==4.9.1) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/pyenv/lib/python3.8/site-packages (from requests->transformers==4.9.1) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/pyenv/lib/python3.8/site-packages (from requests->transformers==4.9.1) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/ubuntu/pyenv/lib/python3.8/site-packages (from requests->transformers==4.9.1) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.9.1 ruamel.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "aifQ6tGqVdbH"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import ruamel.yaml\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8T7z6GOlVdbN"
   },
   "outputs": [],
   "source": [
    "config_name = './genetic_algorithm/settings.yaml'\n",
    "\n",
    "with open(config_name, 'r') as stream:\n",
    "    try:\n",
    "        yaml = ruamel.yaml.YAML()\n",
    "        config = yaml.load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "#  New directory for current genetic algorithm\n",
    "directory = config['Paths']['iteration_folder']\n",
    "\n",
    "#  Path to scored aptamers\n",
    "aptamerList = config['Paths']['path_to_initial_aptamers']\n",
    "aptamerListAll = config['Paths']['path_to_all_aptamers']\n",
    "#  Path to PyTorch alBERT model\n",
    "path_to_model = config['Paths']['path_to_model']\n",
    "\n",
    "#  How many sequences we want to have in a list\n",
    "apt_len = config['Parameters']['aptamer_len']\n",
    "\n",
    "aptamerList_iter = './datasets/ga_interim_data/Albumin/breed_1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hz6g0s47VdbR"
   },
   "source": [
    "Stage I\n",
    "Load model and create a DataLouder for latter GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ny955MP9VdbS"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, maxlen, with_labels=True, bert_model='albert-base-v2'):\n",
    "        self.data = data  # pandas dataframe\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model, return_dict=False)  \n",
    "        self.maxlen = maxlen\n",
    "        self.with_labels = with_labels \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sent1 = str(self.data.loc[index, 'Sequence1'])\n",
    "        sent2 = str(self.data.loc[index, 'Sequence2'])\n",
    "\n",
    "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
    "        encoded_pair = self.tokenizer(sent1, sent2, \n",
    "                                      padding='max_length',  # Pad to max_length\n",
    "                                      truncation=True,  # Truncate to max_length\n",
    "                                      max_length=self.maxlen,  \n",
    "                                      return_tensors='pt')  # Return torch.Tensor objects\n",
    "        \n",
    "        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n",
    "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
    "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
    "\n",
    "        if self.with_labels:  # True if the dataset has labels\n",
    "            label = self.data.loc[index, 'Label']\n",
    "            return token_ids, attn_masks, token_type_ids, label  \n",
    "        else:\n",
    "            return token_ids, attn_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "yVDF_6wsVdbU"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert_layer = AutoModel.from_pretrained(bert_model, return_dict=False)\n",
    "\n",
    "        bert_model == \"albert-base-v2\"  # 12M parameters\n",
    "        hidden_size = 768\n",
    "        #  More information on available models can be found at https://huggingface.co/transformers/pretrained_models.html\n",
    "        \n",
    "        # Freeze model layers and only train the classification layer weights\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # Putting Classification layer on top of BERT\n",
    "        self.cls_layer = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    @autocast()  # Mixes precision\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -input_ids : Tensor  containing token ids\n",
    "            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n",
    "            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n",
    "        '''\n",
    "\n",
    "        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n",
    "        cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)\n",
    "\n",
    "        # Feeding to the classifier layer the last layer hidden-state of the [CLS] token further processed by a\n",
    "        # Linear Layer and a Tanh activation. The Linear layer weights were trained from the sentence order prediction (ALBERT) or next sentence prediction (BERT)\n",
    "        # objective during pre-training.\n",
    "        logits = self.cls_layer(self.dropout(pooler_output))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "BIfXHMI8VdbW"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\" Set all seeds to make results reproducible \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9PmAebATVdbX"
   },
   "outputs": [],
   "source": [
    "def get_probs_from_logits(logits):\n",
    "    \"\"\"\n",
    "    Converts a tensor of logits into an array of probabilities by applying the sigmoid function\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    return probs.detach().cpu().numpy()\n",
    "\n",
    "def test_prediction(net, device, aptamerDataFrame, dataloader, with_labels, result_path, iteration):\n",
    "    \"\"\"\n",
    "    Predict the probabilities on a dataset with or without labels and print the result in a file\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    probs_all = []\n",
    "    nb_iterations = len(dataloader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if with_labels:\n",
    "            for it, (seq, attn_masks, token_type_ids) in tqdm(enumerate(dataloader), total = nb_iterations):\n",
    "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
    "                logits = net(seq, attn_masks, token_type_ids)\n",
    "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
    "                probs_all += probs.tolist()\n",
    "\n",
    "                \n",
    "        else:\n",
    "            for it, (seq, attn_masks, token_type_ids) in tqdm(enumerate(dataloader), total=nb_iterations):\n",
    "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
    "                logits = net(seq, attn_masks, token_type_ids)\n",
    "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
    "                probs_all += probs.tolist()\n",
    "\n",
    "                \n",
    "    df1 = pd.read_csv(aptamerDataFrame)\n",
    "    probs_all = [round(x) for x in probs_all]\n",
    "    df2 = pd.DataFrame({'Label': probs_all})\n",
    "    df = pd.concat([df1, df2], axis=1)\n",
    "    df.to_csv(result_path)\n",
    "    \n",
    "    print(\"Compared aptamers iteration {} is located in {}\".format(iteration, result_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kR33Azc3VdbX"
   },
   "outputs": [],
   "source": [
    "bert_model =  config['Model']['model_name']  #  'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2' and others\n",
    "maxlen =  config['Model']['max_len']         #  maximum length of the tokenized input sentence pair : if greater than \"maxlen\", the input is truncated and else if smaller, the input is padded\n",
    "bs =  config['Model']['batch_size']          #  batch size of testing\n",
    "with_labels =  config['Model']['with_labels']\n",
    "iter = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD29WGchVdbY"
   },
   "source": [
    "Stage III\n",
    "Apply Genetic Algorithm to generate new population of aptamers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dKS3yQJpVdbY"
   },
   "outputs": [],
   "source": [
    "def run_GA():\n",
    "    iter = 1\n",
    "    while iter < 51:\n",
    "      #  Generate N aptamers to have the same 1000 as before deleting inferior\n",
    "      !python ./genetic_algorithm/breeder.py --p {aptamerList} --o {directory} --l {apt_len} --i {iter}\n",
    "      \n",
    "      if iter > 1:\n",
    "        breedCSV = './datasets/ga_interim_data/Albumin/breed_{}.csv'.format(iter-1)\n",
    "        %rm $breedCSV\n",
    "\n",
    "      #  Pair up new batch\n",
    "      !python ./functions/pairing.py --h {aptamerList_iter} --o {directory} --i {iter}\n",
    "\n",
    "      #  Call alBERT to compare goodness of sequences\n",
    "      df_test = pd.read_csv('{}iteration_{}.csv'.format(directory, iter))\n",
    "      test_set = CustomDataset(df_test, maxlen, with_labels, bert_model)\n",
    "      data_toModel = DataLoader(test_set, batch_size=bs) #nureadinti data pirma\n",
    "      test_prediction(net=model, device=device, aptamerDataFrame='{}iteration_{}.csv'.format(directory, iter), dataloader=data_toModel, with_labels=False, result_path='{}predicted_{}.csv'.format(directory, iter), iteration=iter)\n",
    "\n",
    "      #  Find dominating aptamers and go to step 1 again.\n",
    "      !python ./functions/dominance_score.py --p {directory + 'predicted_' +str(iter) + '.csv'} --f {directory + 'breed_' + str(iter) + '.csv'} --o {directory + 'top_iter_' + str(iter)}  --i {iter} --l {apt_len}\n",
    "      #survarkyti kur galunes nera tokios ir tegul patys scriptai tuo rupinasi\n",
    "\n",
    "      iterationCSV = './datasets/ga_interim_data/Albumin/iteration_{}.csv'.format(iter)\n",
    "      predictionCSV = './datasets/ga_interim_data/Albumin/predicted_{}.csv'.format(iter)\n",
    "\n",
    "      aptamerList = directory + 'top_iter_' + str(iter) + '.csv'\n",
    "      iter += 1\n",
    "      aptamerList_iter = './datasets/ga_interim_data/Albumin/breed_{}.csv'.format(iter)\n",
    "\n",
    "      !rm $iterationCSV $predictionCSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TyKSDhrbVdbZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New generation saved in  ./datasets/ga_interim_data/Albumin/\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"./functions/pairing.py\", line 123, in <module>\n",
      "    main()\n",
      "  File \"./functions/pairing.py\", line 117, in main\n",
      "    dataset = pairWithoutLabel()\n",
      "  File \"./functions/pairing.py\", line 55, in pairWithoutLabel\n",
      "    apt1 = apt1.append([x]*(len(df) - first - 1), ignore_index = True)\n",
      "  File \"/home/ubuntu/pyenv/lib/python3.8/site-packages/pandas/core/frame.py\", line 8961, in append\n",
      "    concat(\n",
      "  File \"/home/ubuntu/pyenv/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/pyenv/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 307, in concat\n",
      "    return op.get_result()\n",
      "  File \"/home/ubuntu/pyenv/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 532, in get_result\n",
      "    new_data = concatenate_managers(\n",
      "  File \"/home/ubuntu/pyenv/lib/python3.8/site-packages/pandas/core/internals/concat.py\", line 209, in concatenate_managers\n",
      "    elif _is_uniform_join_units(join_units):\n",
      "  File \"/home/ubuntu/pyenv/lib/python3.8/site-packages/pandas/core/internals/concat.py\", line 607, in _is_uniform_join_units\n",
      "    all(not ju.is_na or ju.block.is_extension for ju in join_units)\n",
      "  File \"/home/ubuntu/pyenv/lib/python3.8/site-packages/pandas/core/internals/concat.py\", line 607, in <genexpr>\n",
      "    all(not ju.is_na or ju.block.is_extension for ju in join_units)\n",
      "  File \"pandas/_libs/properties.pyx\", line 37, in pandas._libs.properties.CachedProperty.__get__\n",
      "  File \"/home/ubuntu/pyenv/lib/python3.8/site-packages/pandas/core/internals/concat.py\", line 401, in is_na\n",
      "    return isna_all(values_flat)\n",
      "  File \"/home/ubuntu/pyenv/lib/python3.8/site-packages/pandas/core/dtypes/missing.py\", line 674, in isna_all\n",
      "    return all(\n",
      "  File \"/home/ubuntu/pyenv/lib/python3.8/site-packages/pandas/core/dtypes/missing.py\", line 679, in <genexpr>\n",
      "    checker(arr[i : i + chunk_len]).all()  # type: ignore[arg-type]\n",
      "  File \"/home/ubuntu/pyenv/lib/python3.8/site-packages/pandas/core/dtypes/missing.py\", line 670, in <lambda>\n",
      "    checker = lambda x: _isna_array(  # type: ignore[assignment]\n",
      "  File \"/home/ubuntu/pyenv/lib/python3.8/site-packages/pandas/core/dtypes/missing.py\", line 246, in _isna_array\n",
      "    result = _isna_string_dtype(values, inf_as_na=inf_as_na)\n",
      "  File \"/home/ubuntu/pyenv/lib/python3.8/site-packages/pandas/core/dtypes/missing.py\", line 271, in _isna_string_dtype\n",
      "    vec = libmissing.isnaobj(values.ravel())\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets/ga_interim_data/Albumin/iteration_1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3215/2976023411.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#tikriausiai nereikia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrun_GA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3215/2924490402.py\u001b[0m in \u001b[0;36mrun_GA\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0;31m#  Call alBERT to compare goodness of sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}iteration_{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m       \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mdata_toModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#nureadinti data pirma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets/ga_interim_data/Albumin/iteration_1.csv'"
     ]
    }
   ],
   "source": [
    "set_seed(2021)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model(bert_model, freeze_bert=False)\n",
    "model.load_state_dict(torch.load(config['Paths']['path_to_model']))\n",
    "model.to(device)\n",
    "model.eval() #tikriausiai nereikia\n",
    "\n",
    "run_GA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GA.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "3aee3b0eb6142d763d7754525c5291c780d2cfe01d72d8f6b94276ca950be07b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
