{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ALBERT for aptamer-pair classification"
   ],
   "metadata": {
    "id": "TuLR0BMJhIub"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set dependancies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.onnx\r\n",
    "import os\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import copy\r\n",
    "import torch.optim as optim\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import ruamel.yaml\r\n",
    "from torch.utils.data import DataLoader, Dataset\r\n",
    "from torch.cuda.amp import autocast, GradScaler\r\n",
    "from tqdm import tqdm\r\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\r\n",
    "import transformers\r\n",
    "import logging\r\n",
    "from transformers import logging\r\n",
    "\r\n",
    "logging.set_verbosity_error() #  to skip errors\r\n",
    "\r\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\r\n",
    "config_name = 'config.yaml'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(config_name, 'r') as stream:\r\n",
    "    try:\r\n",
    "        yaml = ruamel.yaml.YAML()\r\n",
    "        config = yaml.load(stream)\r\n",
    "    except yaml.YAMLError as exc:\r\n",
    "        print(exc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load your dataset"
   ],
   "metadata": {
    "id": "lpKx43Iq6znw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "path_to_train_data = config['Datasets']['train']\r\n",
    "path_to_val_data = config['Datasets']['val']\r\n",
    "path_to_test_data = config['Datasets']['test']\r\n",
    "\r\n",
    "\r\n",
    "df_train = pd.read_csv(path_to_train_data)\r\n",
    "df_val = pd.read_csv(path_to_val_data)\r\n",
    "df_test = pd.read_csv(path_to_test_data)\r\n",
    "\r\n",
    "#  Check the data\r\n",
    "print(df_train.shape, df_val.shape, df_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df_train.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence1</th>\n",
       "      <th>Sequence2</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GCGGTAACCGGTGCT</td>\n",
       "      <td>GGAGAACAACAACTT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATCAGATACTATGCG</td>\n",
       "      <td>TACAGAACTAGCAGG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGAGTGCCCTGTCCC</td>\n",
       "      <td>ATCTGCTAACCAGTC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATTCCCAAGCTGCGA</td>\n",
       "      <td>GTACCCGGGATGTTA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GCGAGCTCATGCACA</td>\n",
       "      <td>GGGCGCCATAGTTCG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sequence1        Sequence2  Label\n",
       "0  GCGGTAACCGGTGCT  GGAGAACAACAACTT      0\n",
       "1  ATCAGATACTATGCG  TACAGAACTAGCAGG      0\n",
       "2  AGAGTGCCCTGTCCC  ATCTGCTAACCAGTC      0\n",
       "3  ATTCCCAAGCTGCGA  GTACCCGGGATGTTA      0\n",
       "4  GCGAGCTCATGCACA  GGGCGCCATAGTTCG      0"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "irj7itV0UCF_",
    "outputId": "a648d128-a509-4cf7-8b27-7474d83a50dd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classes and functions"
   ],
   "metadata": {
    "id": "TWJfh6DV7CB5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class CustomDataset(Dataset):\r\n",
    "\r\n",
    "    def __init__(self, data, maxlen=32, with_labels=True, bert_model='albert-base-v2'):\r\n",
    "        self.data = data \r\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model, return_dict=False)  \r\n",
    "        self.maxlen = maxlen\r\n",
    "        self.with_labels = with_labels \r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.data)\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        seq1 = str(self.data.loc[index, 'Sequence1'])\r\n",
    "        seq2 = str(self.data.loc[index, 'Sequence2'])\r\n",
    "\r\n",
    "        # Tokenizing input of two sequences \"sentence\" to get token ids, attention masks and token type ids\r\n",
    "        encoded_pair = self.tokenizer(seq1, seq2, \r\n",
    "                                      padding='max_length',  #  Pad input if tokenized input is shorter than max_length\r\n",
    "                                      truncation=True,  #  Truncate in case tokenized input surpasses length 32\r\n",
    "                                      max_length=self.maxlen,  \r\n",
    "                                      return_tensors='pt')  #  Return torch.Tensor objects\r\n",
    "        \r\n",
    "        #  Tensors \r\n",
    "        token_ids = encoded_pair['input_ids'].squeeze(0)  #  Of token ids\r\n",
    "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  #  \"0\" - padded values and \"1\" - other values\r\n",
    "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  #  \"0\" - 1st sentence tokens, \"1\" - 2nd sentence tokens\r\n",
    "\r\n",
    "        if self.with_labels:  # True if the dataset has labels, for training or evaluating models goodness metrics, where we have labels of \"correct class\"\r\n",
    "            label = self.data.loc[index, 'Label']\r\n",
    "            return token_ids, attn_masks, token_type_ids, label  \r\n",
    "        else: #  In case we are running prediction on never seen aptamers\r\n",
    "            return token_ids, attn_masks, token_type_ids"
   ],
   "outputs": [],
   "metadata": {
    "id": "Tc1GQh7yEm4C"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class Model(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False):\r\n",
    "        super(Model, self).__init__()\r\n",
    "        self.bert_layer = AutoModel.from_pretrained(bert_model, return_dict=False)\r\n",
    "        self.config = self.bert_layer.config #  this line helps to save its configs later on\r\n",
    "        \r\n",
    "        #  More information on number of models hidden size can be found https://huggingface.co/transformers/pretrained_models.html\r\n",
    "        hidden_size = config['Model']['hidden_size']\r\n",
    "\r\n",
    "        #  Freeze every bert layer except the classification layer \r\n",
    "        if freeze_bert:\r\n",
    "            for l in self.bert_layer.parameters():\r\n",
    "                l.requires_grad = False\r\n",
    "\r\n",
    "        #  Classification layer\r\n",
    "        self.cls_layer = nn.Linear(hidden_size, config['Model']['number_labels'])\r\n",
    "        self.dropout = nn.Dropout(p=config['Model']['dropout_rate'])\r\n",
    "\r\n",
    "    @autocast()  #  run in mixed precision, it helps to save some precious time :)\r\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids):\r\n",
    "\r\n",
    "        #  Feeding the inputs of `CustomDataset` to the BERT-based model to obtain contextualized representations\r\n",
    "        cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)\r\n",
    "\r\n",
    "        #  Feeding to the classifier layer the last layer hidden-state of the [CLS] token further processed by a\r\n",
    "        #  Linear Layer and a Tanh activation (which determines the class of aptamers pair)\r\n",
    "        logits = self.cls_layer(self.dropout(pooler_output))\r\n",
    "\r\n",
    "        return logits"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#  Seed helps to recreate your results\r\n",
    "def set_seed(seed):\r\n",
    "    torch.manual_seed(seed)\r\n",
    "    torch.cuda.manual_seed_all(seed)\r\n",
    "    torch.backends.cudnn.deterministic = True\r\n",
    "    torch.backends.cudnn.benchmark = False\r\n",
    "    np.random.seed(seed)\r\n",
    "    random.seed(seed)\r\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\r\n",
    "    \r\n",
    "\r\n",
    "def evaluate_loss(net, device, criterion, dataloader):\r\n",
    "    net.eval()\r\n",
    "    mean_loss = 0\r\n",
    "    count = 0\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):\r\n",
    "            seq, attn_masks, token_type_ids, labels = \\\r\n",
    "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\r\n",
    "            logits = net(seq, attn_masks, token_type_ids)\r\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\r\n",
    "            count += 1\r\n",
    "\r\n",
    "    return mean_loss / count"
   ],
   "outputs": [],
   "metadata": {
    "id": "5SrSNNYTjwe8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):\r\n",
    "    best_loss = np.Inf\r\n",
    "    nb_iterations = len(train_loader)\r\n",
    "    print_every = nb_iterations // config['Random']['print_every']  #  To print fine-tuning results less often if needed\r\n",
    "    scaler = GradScaler()\r\n",
    "    \r\n",
    "    iters = []\r\n",
    "    train_losses = []\r\n",
    "    val_losses = []\r\n",
    "\r\n",
    "    for ep in range(epochs):\r\n",
    "\r\n",
    "        net.train()\r\n",
    "        running_loss = 0.0\r\n",
    "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(train_loader)):\r\n",
    "\r\n",
    "            # Converting to cuda tensors\r\n",
    "            seq, attn_masks, token_type_ids, labels = \\\r\n",
    "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\r\n",
    "    \r\n",
    "            # Enables autocasting for the forward pass (model + loss)\r\n",
    "            with autocast():\r\n",
    "                # Obtaining the logits from the model\r\n",
    "                logits = net(seq, attn_masks, token_type_ids)\r\n",
    "\r\n",
    "                # Computing loss\r\n",
    "                loss = criterion(logits.squeeze(-1), labels.float())\r\n",
    "                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged\r\n",
    "\r\n",
    "            # Backpropagating the gradients. Calls backward() on scaled loss to create scaled gradients.\r\n",
    "            scaler.scale(loss).backward()\r\n",
    "\r\n",
    "            if (it + 1) % iters_to_accumulate == 0:\r\n",
    "                # Optimization step\r\n",
    "                scaler.step(opti)\r\n",
    "                # Updates the scale for next iteration.\r\n",
    "                scaler.update()\r\n",
    "                # Adjust the learning rate based on the number of iterations.\r\n",
    "                lr_scheduler.step()\r\n",
    "                # Clear gradients\r\n",
    "                opti.zero_grad()\r\n",
    "\r\n",
    "            running_loss += loss.item()\r\n",
    "\r\n",
    "            if (it + 1) % print_every == 0:  # Print training loss information\r\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\r\n",
    "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\r\n",
    "                \r\n",
    "                train_losses.append(running_loss/print_every)\r\n",
    "                running_loss = 0.0\r\n",
    "\r\n",
    "\r\n",
    "        val_loss = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss\r\n",
    "        val_losses.append(val_loss)\r\n",
    "        \r\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\r\n",
    "\r\n",
    "        if val_loss < best_loss:\r\n",
    "\r\n",
    "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\r\n",
    "            net_copy = copy.deepcopy(net)  # save a copy of the model\r\n",
    "            best_loss = val_loss\r\n",
    "        \r\n",
    "    # Saving the model\r\n",
    "    path_to_model='model/{}_{}_{}.pt'.format(bert_model, lr, round(best_loss, 4)) #  renames path of model to instantly use it to evaluate metrics\r\n",
    "    path_to_model_evaluation = './datasets/model_validation/{}_{}_{}.csv'.format(bert_model, lr, round(best_loss, 4)) # Path to model fine-tuning losses\r\n",
    "    path_to_model_test = './datasets/model_validation/pred-{}_{}_{}.csv'.format(bert_model, lr, round(best_loss, 4)) #  Path to model predicted test dataset\r\n",
    "    \r\n",
    "    #  Following val/test losses to graph it later\r\n",
    "    model_training = './datasets/model_validation/{}_{}_{}_training.csv'.format(bert_model, lr, round(best_loss, 4))\r\n",
    "    losses = pd.DataFrame({'Train loss': train_losses, 'Val loss': val_losses})\r\n",
    "    losses.to_csv(model_training)\r\n",
    "    \r\n",
    "    config['Random']['path_to_model'] = path_to_model\r\n",
    "    config['Random']['path_to_model_evaluation'] = path_to_model_evaluation\r\n",
    "    config['Random']['path_to_model_test'] = path_to_model_test\r\n",
    "\r\n",
    "    torch.save(net_copy.state_dict(), path_to_model)\r\n",
    "    print(\"The model has been saved in {}\".format(path_to_model))\r\n",
    "\r\n",
    "    #  To save PyTorch model to .bin file with config.json run:\r\n",
    "    #configs = net.congif\r\n",
    "    #net.save_pretrained('Model-Directory')\r\n",
    "    \r\n",
    "    del loss\r\n",
    "    torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "metadata": {
    "id": "I-o6KyaFkU5u"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters for training"
   ],
   "metadata": {
    "id": "WFy9kQ2-SvQ2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "bert_model = config['Model']['bert_model']  # any of previously defined BERT alternatives :'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2', 'albert-xxlarge-v2' and others\r\n",
    "freeze_bert = config['Model']['freeze_bert']  # if True, freeze the encoder weights and only update the classification layer weights\r\n",
    "maxlen = config['Model']['max_len']        # maximum length of the tokenized input sentence pair : if greater than \"maxlen\", the input is truncated and else if smaller, the input is padded\r\n",
    "bs = config['Model']['batch_size']                # batch size\r\n",
    "iters_to_accumulate = config['Model']['iters_to_accumulate']  # the gradient accumulation adds gradients over an effective batch of size : bs * iters_to_accumulate. If set to \"1\", you get the usual batch size\r\n",
    "lr = config['Model']['learning_rate']                # learning rate\r\n",
    "epochs = config['Model']['epochs']             # number of training epochs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  Set all seeds to make reproducible results\r\n",
    "set_seed(2021)\r\n",
    " \r\n",
    "#  Creating instances of training and validation set\r\n",
    "print(\"Reading training and validation data...\")\r\n",
    "train_set = CustomDataset(df_train, maxlen, bert_model)\r\n",
    "val_set = CustomDataset(df_val, maxlen, bert_model)\r\n",
    "\r\n",
    "#  Creating isntances for model training\r\n",
    "train_loader = DataLoader(train_set, batch_size=bs, num_workers=2)\r\n",
    "val_loader = DataLoader(val_set, batch_size=bs, num_workers=2)\r\n",
    " \r\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "net = Model(bert_model, freeze_bert=freeze_bert)\r\n",
    "\r\n",
    "#  In case you have more than 1 GPU\r\n",
    "if torch.cuda.device_count() > 1:\r\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\r\n",
    "    net = nn.DataParallel(net)\r\n",
    "\r\n",
    "net.to(device)\r\n",
    "   \r\n",
    "criterion = nn.BCEWithLogitsLoss()\r\n",
    "opti = AdamW(net.parameters(), lr=lr, weight_decay=1e-2) #  More on AdamW can be found https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e \r\n",
    "num_warmup_steps = config['Model']['num_warmup_steps'] #  The number of steps for the warmup phase.\r\n",
    "num_training_steps = epochs * len(train_loader)  #  The total number of training steps\r\n",
    "t_total = (len(train_loader) // iters_to_accumulate) * epochs  #  Necessary to take into account Gradient accumulation\r\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)\r\n",
    "                         \r\n",
    "train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate)\r\n",
    "\r\n",
    "#  Dump changed configuration parameters in train_bert\r\n",
    "with open('config.yaml', 'w') as conf:\r\n",
    "    yaml.dump(config, conf)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true,
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#  Simply converting logit tensors to probabilities arrays\r\n",
    "def get_probs_from_logits(logits):\r\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\r\n",
    "    return probs.detach().cpu().numpy()\r\n",
    "\r\n",
    "#  Predict from probabilities and move results to csv\r\n",
    "def test_prediction(net, device, aptamerDataFrame, dataloader, with_labels, result_path=config['Random']['path_to_model_test']):\r\n",
    "    net.eval()\r\n",
    "    probs_all = []\r\n",
    "    nb_iterations = len(dataloader)\r\n",
    "    \r\n",
    "    with torch.no_grad():\r\n",
    "        if with_labels:\r\n",
    "            for it, (seq, attn_masks, token_type_ids, label) in enumerate(tqdm(dataloader)):\r\n",
    "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\r\n",
    "                logits = net(seq, attn_masks, token_type_ids)\r\n",
    "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\r\n",
    "                probs_all += probs.tolist()\r\n",
    "                \r\n",
    "        else:\r\n",
    "            for it, (seq, attn_masks, token_type_ids, token_ids) in enumerate(tqdm(dataloader)):\r\n",
    "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\r\n",
    "                logits = net(seq, attn_masks, token_type_ids)\r\n",
    "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\r\n",
    "                probs_all += probs.tolist()\r\n",
    "            \r\n",
    "    y_hat = [round(x) for x in probs_all]     \r\n",
    "    df2 = pd.DataFrame({'y_hat': y_hat, 'prob': probs_all})\r\n",
    "    df = pd.concat([aptamerDataFrame, df2], axis=1)\r\n",
    "    df.to_csv(result_path)\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "m3r8_npVf30D"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "\r\n",
    "print(\"Reading test data...\")\r\n",
    "test_set = CustomDataset(df_test, maxlen, bert_model)\r\n",
    "test_loader = DataLoader(test_set, batch_size=bs, num_workers=4)\r\n",
    "\r\n",
    "model = Model(bert_model)\r\n",
    "if torch.cuda.device_count() > 1:  # if multiple GPUs\r\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\r\n",
    "    model = nn.DataParallel(model)\r\n",
    "\r\n",
    "print(\"Loading the weights of the model...\")\r\n",
    "model.load_state_dict(torch.load(config['Random']['path_to_model']))\r\n",
    "model.to(device)\r\n",
    "\r\n",
    "print(\"Predicting on test data...\")\r\n",
    "test_prediction(net=model\r\n",
    "                , device=device\r\n",
    "                , dataloader=test_loader\r\n",
    "                , aptamerDataFrame=df_test\r\n",
    "                , with_labels = True)\r\n",
    "\r\n",
    "print(\"Predictions are available in : {}\".format(config['Random']['path_to_model_test']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Convert model to ONNX\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if config['Model']['convert_to_onnx']:\r\n",
    "    sequences = {'AGCTAAGCTAAGCTA':'AAGACTGACAGCTAA'}\r\n",
    "    sequences=pd.DataFrame(sequences.items(), columns=['Sequence1', 'Sequence2'])\r\n",
    "    dataset = CustomDataset(\r\n",
    "        data = sequences,\r\n",
    "        maxlen = config['Model']['max_len'],\r\n",
    "        with_labels = False #sulyginti su class cusotmedatasets\r\n",
    "        )\r\n",
    "    \r\n",
    "    model = Model(bert_model)\r\n",
    "    \r\n",
    "    model.load_state_dict(torch.load(config['Random']['path_to_model']))\r\n",
    "    model.to(device)\r\n",
    "    model.eval()\r\n",
    "    #defining model inputs, which are ids, kmask and toke type ids (it comes from Model Class)\r\n",
    "    input_ids= dataset[0][0].unsqueeze(0).cuda()\r\n",
    "    attn_masks = dataset[0][0].unsqueeze(0).cuda()\r\n",
    "    token_type_ids = dataset[0][0].unsqueeze(0).cuda()\r\n",
    "        \r\n",
    "    torch.onnx.export(\r\n",
    "        model, #.module if paralized\r\n",
    "        (input_ids, attn_masks, token_type_ids),\r\n",
    "        \"albert-base-albumin.onnx\",\r\n",
    "        input_names=[\"input_ids\", \"attn_masks\", \"token_type_ids\"], \r\n",
    "        output_names=[\"output\"],\r\n",
    "        #which inputs have dynamical axes\r\n",
    "        verbose=True,\r\n",
    "        dynamic_axes={\r\n",
    "            \"input_ids\": {0: \"batch_size\"},\r\n",
    "            \"attn_masks\": {0: \"batch_size\"},\r\n",
    "            \"token_type_ids\": {0: \"batch_size\"},\r\n",
    "            \"output\": {0: \"batch_size\"},\r\n",
    "        },\r\n",
    "        opset_version=10,\r\n",
    "        )    "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}